{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Packages and Libraries"
      ],
      "metadata": {
        "id": "x9YgJOamhULR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate transformers[torch]\n",
        "!pip install py7zr"
      ],
      "metadata": {
        "id": "R23Rk65_NUSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "60Q-H6PPNUUw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and Data Processing"
      ],
      "metadata": {
        "id": "5RFKX5GAhzeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "K5_k1KBGNUXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"samsum\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "mvMHZZXdNUZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = ds[\"test\"][0]['dialogue']\n",
        "label = ds[\"test\"][0]['summary']\n",
        "token_ids = tokenizer(sample, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "O9QouP8uQVc4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_ids = model.generate(token_ids['input_ids'], min_length=30, max_length=250)"
      ],
      "metadata": {
        "id": "uz3wY-t8xzEa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "vFrKV5SYxzNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_summary(sample, label, model):\n",
        "  prompt = f\"\"\"Summarize this dialogue: {sample}.\n",
        "  Summary:\n",
        "  \"\"\"\n",
        "  token_ids = tokenizer(sample, return_tensors=\"pt\")\n",
        "  summary_ids = model.generate(token_ids['input_ids'], min_length=30, max_length=250)\n",
        "  output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "print(f\"Sample: {sample}\")\n",
        "print(f\"Label: {label}\")\n",
        "print(f\"Model Output: {prompt_summary(sample, label, model)}\")"
      ],
      "metadata": {
        "id": "OUntIF-XNUcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(inputs):\n",
        "  sp = \"summarize: \\n\\n \"\n",
        "  ep = \"\\n\\nSummary: \"\n",
        "  prompt = [sp+dialogue+ep for dialogue in inputs[\"dialogue\"]]\n",
        "  inputs['input_ids'] = tokenizer(prompt, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512).input_ids\n",
        "  inputs['labels'] = tokenizer(inputs[\"summary\"], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512).input_ids\n",
        "  return inputs\n"
      ],
      "metadata": {
        "id": "6O9Szkm4NUei"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenized_ds = ds.map(tokenize, batched=True)\n",
        "tokenized_ds = tokenized_ds.remove_columns([\"id\", \"dialogue\", \"summary\"])\n",
        "tokenized_ds = tokenized_ds.filter(lambda x, y: y % 50 == 0, with_indices=True)\n",
        "print(tokenized_ds['train'].shape)\n",
        "print(tokenized_ds['validation'].shape)\n",
        "print(tokenized_ds['test'].shape)\n",
        "print(tokenized_ds['train'][0].keys())"
      ],
      "metadata": {
        "id": "-MYM0fy6NUhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "slupBtefyVRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation"
      ],
      "metadata": {
        "id": "DXK0u3LKiE0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = TrainingArguments(\n",
        "    output_dir=\"./dialogue_bart\",\n",
        "    learning_rate=1e-5,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=2,)\n",
        "\n",
        "trainer = Trainer(model, train_args, train_dataset=tokenized_ds[\"train\"], eval_dataset=tokenized_ds[\"validation\"])"
      ],
      "metadata": {
        "id": "CII6227gNUjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "RrzCg9afpydv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer.save_model(\"/content/dialogue_bart\")\n",
        "trainer.push_to_hub(\"dialogue_bart\")"
      ],
      "metadata": {
        "id": "tc4YnCT9tRK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/dialogue_bart\")\n",
        "tuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"ibraheemaloran/dialogue_bart\")"
      ],
      "metadata": {
        "id": "OWMVgB4Rtaw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  sample = ds[\"test\"][i]\n",
        "  print(f\"Sample: {sample['dialogue']}\")\n",
        "  print(f\"Label: {sample['summary']}\")\n",
        "  print(f\"Model Output: {prompt_summary(sample['dialogue'], sample['summary'], tuned_model)}\")"
      ],
      "metadata": {
        "id": "t3rq6gA0rQXN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}